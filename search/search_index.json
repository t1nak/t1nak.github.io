{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hi there! Overview Ph.D. in Economics University of Cape Town, ZA Research , CV tina.koziol [ at ]alumni.uct.ac.za I'm an Econ PhD graduate from University of Cape Town with an interest in Computational Economics. I'm passionate about integrating data science methods into academic science and teaching. I'm particularly fond of python due to its versatility and easy, beautiful syntax. Maybe I can convince you to learn python here . \ud83e\udd17 NEW Check out new blog post Python module for accessibility mapping using google earth engine available at pypi Python helper class to analyse BA900 forms of South African the Reserve Bank available on github Work in progress -->click for details Koziol, 2020 (SARB Working paper): Stress-testing interconnected portfolios in the South African Banking Sector Davids, du Rand, Georg, Koziol & Schasfoort, 2020 (Working paper): SABCoM: A Spatial Agent-Based COVID-19 Model Source code: https://github.com/blackrhinoabm/sabcom Koziol, Riedler, 2019 (Working paper): Euro Area, Quantitative Easing in a Portfolio Balance Model with Heterogeneous Agents and Assets Teaching UCT, 2020, School of Economics, Lecturer Econometrics (Master and PhD level ECO5046F) UCT, 2019, School of Economics, Lecturer Python for Economics, short course for PhD students UCT, 2019, School of Economics, Lecturer Quantitative methods in Economics, BSc degree UCT, 2018 & 2017, African Institute for Financial Market and Risk Management, TA Econometrics, MCom degree Seminars & Talks Research Seminar at the South African Reserve Bank, 9 November 2020 Economics PhD Conference at Stellenbosch University, 11 - 12 July 2019 Closing Conference of the Project Quantitative Easing and Financial (In)Stability funded, Frankfurt am Main, Germany, 1 April 2019 Financial Stability Workshop of South African Reserve Bank, Pretoria, South Africa, 30th October 2017 INET/Oxford - UCT Workshop on Agent-Based Modelling for Systemic Stress Testing, Oxford, United Kingdom 8-10 March 2017 Tools We are developing BlackRhino , an open source agent-based model of the financial system in python. I use BlackRhino to implement a model of asset fire-sale propagation across the South African banking sector.","title":"Home"},{"location":"#hi-there","text":"Overview Ph.D. in Economics University of Cape Town, ZA Research , CV tina.koziol [ at ]alumni.uct.ac.za I'm an Econ PhD graduate from University of Cape Town with an interest in Computational Economics. I'm passionate about integrating data science methods into academic science and teaching. I'm particularly fond of python due to its versatility and easy, beautiful syntax. Maybe I can convince you to learn python here . \ud83e\udd17","title":"Hi there!"},{"location":"#new","text":"Check out new blog post Python module for accessibility mapping using google earth engine available at pypi Python helper class to analyse BA900 forms of South African the Reserve Bank available on github","title":"NEW"},{"location":"#work-in-progress-click-for-details","text":"Koziol, 2020 (SARB Working paper): Stress-testing interconnected portfolios in the South African Banking Sector Davids, du Rand, Georg, Koziol & Schasfoort, 2020 (Working paper): SABCoM: A Spatial Agent-Based COVID-19 Model Source code: https://github.com/blackrhinoabm/sabcom Koziol, Riedler, 2019 (Working paper): Euro Area, Quantitative Easing in a Portfolio Balance Model with Heterogeneous Agents and Assets","title":"Work in progress --&gt;click for details"},{"location":"#teaching","text":"UCT, 2020, School of Economics, Lecturer Econometrics (Master and PhD level ECO5046F) UCT, 2019, School of Economics, Lecturer Python for Economics, short course for PhD students UCT, 2019, School of Economics, Lecturer Quantitative methods in Economics, BSc degree UCT, 2018 & 2017, African Institute for Financial Market and Risk Management, TA Econometrics, MCom degree","title":"Teaching"},{"location":"#seminars-talks","text":"Research Seminar at the South African Reserve Bank, 9 November 2020 Economics PhD Conference at Stellenbosch University, 11 - 12 July 2019 Closing Conference of the Project Quantitative Easing and Financial (In)Stability funded, Frankfurt am Main, Germany, 1 April 2019 Financial Stability Workshop of South African Reserve Bank, Pretoria, South Africa, 30th October 2017 INET/Oxford - UCT Workshop on Agent-Based Modelling for Systemic Stress Testing, Oxford, United Kingdom 8-10 March 2017","title":"Seminars &amp; Talks"},{"location":"#tools","text":"We are developing BlackRhino , an open source agent-based model of the financial system in python. I use BlackRhino to implement a model of asset fire-sale propagation across the South African banking sector.","title":"Tools"},{"location":"2020/","text":"Download here Last, but not least, my favourite quote driving me forward: A smooth sea never made a skilled sailor. (FDR)","title":"2020"},{"location":"cv/","text":"Download here Last, but not least, my favourite quote driving me forward: A smooth sea never made a skilled sailor. (FDR)","title":"CV"},{"location":"debate/","text":"Debate and Misinformation Understanding Rhetorical Devices and Logical Fallacies Psychology and Philosophy are fascinating fields of study which are concerned with understanding how human beliefs are formed and reinforced. Whether you are conservative, liberal, pro-Trump, pro-Choice or a passionate 'flat-earther' - every belief system is build on a person's individual logic. The rise of social media has shown how 'echo chambers' and hive-minds can contribute to intensifying and radicalising belief systems. If you find yourself lost in the maze of (mis-)information, try to analyse the content that you are reading in terms of Style and rhetorical devises: Is the language appealing to emotions? manipulating? generalising? nuanced? Logical fallacies: Is the argument made sound or is there no argument and just a tool to distract /attack You will find rhetorical tricks and manipulation on both sides on the coin (e.g. pro trump and anti trump), so try to pick the side with the most sound argumentation and language. 1. Pay attention to language 2. Pay attention to logical fallacies Examples: a) Ad hominem: attacking the person instead of their argument This is a very common and (unfortunately) increasingly popular strategy that is nothing more than a distraction scheme. A distraction away from criticisms - it's much much MUCH EASIER to defame/critisise the opponent than actually address the arguments that were brought forward. I could list countless examples here, but do not want to make this post about Trump - it's just a guideline to analyse argumentation styles. b) Paralipsis or Apophasis This one is interesting because it uses a tongue-in-cheek way to bring an opinion across. It's the ultimate 'passive-aggressive' rhetorical device because you are pretending to omit something, but you are actually mentioning it :) For example, in 2009 (wayyyy before covid-19), Michelle Bachmann, a Republican congressional representative, made a comment about the outbreak of swine flu: I find it interesting that it was back in the 1970s that the swine flu broke out (...) under another Democrat president, Jimmy Carter. And I'm not blaming this on President Obama. I just think it's an interesting coincidence. She uses paralipsis to suggest the Democrats are bad at containing the swine flue while simultaneously mentioning that she is not blaming President Obama. But why mention the coincidence if it was not for blaming him? Paralipsis= A figure of speech in which one pretends to ignore or omit something by actually mentioning it The next fallacy is very intuitive and is often 'instrumentalised' by conspiracy theorists to argue for their favour: c) Argumentum ad populum or \"bandwagon fallacy\" A proposition must be true because many or most people believe it. If many believe so, it is true So if someone says: America is exceptional because a lot of people believe America is the greatest country in the world --> it's fallacious. You can bring 100 other reason why the US is a great country, but this one is not a solid argument. What this fallacy is also saying: if a lot of people follow me and my argument, it has a lot of value and must be true. So popularity (crowds, opinion polls, ratings, votes) becomes the main sign of value. It's quite sad that many people need guidance of popularity to make decision. But as Emmanuel Kant' once said: Sapere aude - Dare to think for yourself. d) Argumentum ad ignorantiam - \"a lack of contrary evidence\" The fallacy of appeal to ignorance is when a lack of evidence for one thing leads you to claim that another thing must be true, without interrogating other reasons for the possible invalidity of the other thing. Example: There might be logically sound arguments for using coal or believing in God, but these are not them: \"Science has not definitively shown that the Big Bang ever happened, therefore God exists\". It can't be proved that climate change is due to fossil-fuel emissions, therefore we should carry on using coal. Want more ? Here is an excellent video of 15 logical fallacies committed in 3 minutes of speech by a prominent politician.","title":"Miscellaneous - Debate and Misinformation"},{"location":"debate/#debate-and-misinformation","text":"","title":"Debate and Misinformation"},{"location":"debate/#understanding-rhetorical-devices-and-logical-fallacies","text":"Psychology and Philosophy are fascinating fields of study which are concerned with understanding how human beliefs are formed and reinforced. Whether you are conservative, liberal, pro-Trump, pro-Choice or a passionate 'flat-earther' - every belief system is build on a person's individual logic. The rise of social media has shown how 'echo chambers' and hive-minds can contribute to intensifying and radicalising belief systems. If you find yourself lost in the maze of (mis-)information, try to analyse the content that you are reading in terms of Style and rhetorical devises: Is the language appealing to emotions? manipulating? generalising? nuanced? Logical fallacies: Is the argument made sound or is there no argument and just a tool to distract /attack You will find rhetorical tricks and manipulation on both sides on the coin (e.g. pro trump and anti trump), so try to pick the side with the most sound argumentation and language.","title":"Understanding Rhetorical Devices and Logical Fallacies"},{"location":"debate/#1-pay-attention-to-language","text":"","title":"1. Pay attention to language"},{"location":"debate/#2-pay-attention-to-logical-fallacies","text":"Examples:","title":"2. Pay attention to logical fallacies"},{"location":"debate/#a-ad-hominem-attacking-the-person-instead-of-their-argument","text":"This is a very common and (unfortunately) increasingly popular strategy that is nothing more than a distraction scheme. A distraction away from criticisms - it's much much MUCH EASIER to defame/critisise the opponent than actually address the arguments that were brought forward. I could list countless examples here, but do not want to make this post about Trump - it's just a guideline to analyse argumentation styles.","title":"a) Ad hominem: attacking the person instead of their argument"},{"location":"debate/#b-paralipsis-or-apophasis","text":"This one is interesting because it uses a tongue-in-cheek way to bring an opinion across. It's the ultimate 'passive-aggressive' rhetorical device because you are pretending to omit something, but you are actually mentioning it :) For example, in 2009 (wayyyy before covid-19), Michelle Bachmann, a Republican congressional representative, made a comment about the outbreak of swine flu: I find it interesting that it was back in the 1970s that the swine flu broke out (...) under another Democrat president, Jimmy Carter. And I'm not blaming this on President Obama. I just think it's an interesting coincidence. She uses paralipsis to suggest the Democrats are bad at containing the swine flue while simultaneously mentioning that she is not blaming President Obama. But why mention the coincidence if it was not for blaming him? Paralipsis= A figure of speech in which one pretends to ignore or omit something by actually mentioning it The next fallacy is very intuitive and is often 'instrumentalised' by conspiracy theorists to argue for their favour:","title":"b) Paralipsis or Apophasis"},{"location":"debate/#c-argumentum-ad-populum-or-bandwagon-fallacy","text":"A proposition must be true because many or most people believe it. If many believe so, it is true So if someone says: America is exceptional because a lot of people believe America is the greatest country in the world --> it's fallacious. You can bring 100 other reason why the US is a great country, but this one is not a solid argument. What this fallacy is also saying: if a lot of people follow me and my argument, it has a lot of value and must be true. So popularity (crowds, opinion polls, ratings, votes) becomes the main sign of value. It's quite sad that many people need guidance of popularity to make decision. But as Emmanuel Kant' once said: Sapere aude - Dare to think for yourself.","title":"c) Argumentum ad populum or \"bandwagon fallacy\""},{"location":"debate/#d-argumentum-ad-ignorantiam-a-lack-of-contrary-evidence","text":"The fallacy of appeal to ignorance is when a lack of evidence for one thing leads you to claim that another thing must be true, without interrogating other reasons for the possible invalidity of the other thing. Example: There might be logically sound arguments for using coal or believing in God, but these are not them: \"Science has not definitively shown that the Big Bang ever happened, therefore God exists\". It can't be proved that climate change is due to fossil-fuel emissions, therefore we should carry on using coal. Want more ? Here is an excellent video of 15 logical fallacies committed in 3 minutes of speech by a prominent politician.","title":"d) Argumentum ad ignorantiam    -   \"a lack of contrary evidence\""},{"location":"python/","text":"Python for economists Why should economists learn how to code python? The digital age and data revolution have broadened the appeal of data science methods in both the private sector and academic research. Our modern economy becomes increasingly digitised, while larger computing power and novel data sources provide new opportunities to investigate economic relationships. As a starting point, python programming is particularly apt for research topics surrounding financial and social networks, as well as heterogeneous agent modelling. In addition, it provides powerful data management, visualisation, econometrics, automation tools and serves as easy gateway to more general machine learning applications. Python loves BiG data! What you can do Data management: handling large data sets, cleaning and merging data, visualisation Data analysis: forecasting, investment strategies, geo-mapping, numerical computation, statistical tests and machine learning Automation & Productivity: web-scraping, create LaTeX documents, automate tasks like opening 100 excel files and changing one row Machine learning notebook collection . Beginner python stuff See here","title":"Why learn python"},{"location":"python/#python-for-economists","text":"","title":"Python for economists"},{"location":"python/#why-should-economists-learn-how-to-code-python","text":"The digital age and data revolution have broadened the appeal of data science methods in both the private sector and academic research. Our modern economy becomes increasingly digitised, while larger computing power and novel data sources provide new opportunities to investigate economic relationships. As a starting point, python programming is particularly apt for research topics surrounding financial and social networks, as well as heterogeneous agent modelling. In addition, it provides powerful data management, visualisation, econometrics, automation tools and serves as easy gateway to more general machine learning applications. Python loves BiG data!","title":"Why should economists learn how to code python?"},{"location":"python/#what-you-can-do","text":"Data management: handling large data sets, cleaning and merging data, visualisation Data analysis: forecasting, investment strategies, geo-mapping, numerical computation, statistical tests and machine learning Automation & Productivity: web-scraping, create LaTeX documents, automate tasks like opening 100 excel files and changing one row","title":"What you can do"},{"location":"python/#machine-learning-notebook-collection","text":"","title":"Machine learning notebook collection."},{"location":"python/#beginner-python-stuff","text":"See here","title":"Beginner python stuff"},{"location":"research/","text":"Research Work in progress Davids, du Rand, Georg, Koziol & Schasfoort, 2020 (Working paper) SABCoM - A Spatial Agent-Based COVID-19 Model We develop a Spatial Agent-Based Covid-19 Model (SABCoM). The model contributes to the Covid-19 modelling literature by distinguishing between the district level and city level. The advantage of this approach is that we can study the effect of heterogeneity between districts. This is important in an emerging market context since there are often informal and formal districts that differ both in density and the likely effectiveness of physical distancing policies. Source code, Paper Health Economics Financial Contagion Koziol, 2020 (Working Paper) : Stress-testing interconnected portfolios in the South African Banking Sector Paper This paper calibrates a price-mediated contagion model to the South African banking system. I rank individual banks according to their contribution to systemic risk and show the importance of cash liquidity buffers in reducing risk to fire-sale occurrences. The paper finds a critical threshold price impact parameter, which, if exceeded, makes the banking system highly unstable. source code Paper available here Koziol & Riedler, 2019 (Working paper) : Euro Area Quantitative Easing in a Portfolio Balance Model with Heterogeneous Agents and Assets We present a portfolio model to study the effects of Quantitative Easing on international financial asset returns through the portfolio balance channel. Our two-country model quantifies the effect of a domestic central bank\u2019s asset purchase program on the domestic and foreign term structure of the yield curve, equity returns and the exchange rate. Paper on SSRN","title":"Work in progress"},{"location":"research/#research","text":"","title":"Research"},{"location":"research/#work-in-progress","text":"","title":"Work in progress"},{"location":"research/#davids-du-rand-georg-koziol-schasfoort-2020-working-paper","text":"SABCoM - A Spatial Agent-Based COVID-19 Model We develop a Spatial Agent-Based Covid-19 Model (SABCoM). The model contributes to the Covid-19 modelling literature by distinguishing between the district level and city level. The advantage of this approach is that we can study the effect of heterogeneity between districts. This is important in an emerging market context since there are often informal and formal districts that differ both in density and the likely effectiveness of physical distancing policies. Source code, Paper Health Economics Financial Contagion","title":"Davids, du Rand, Georg, Koziol &amp; Schasfoort, 2020 (Working paper)"},{"location":"research/#koziol-2020-working-paper-stress-testing-interconnected-portfolios-in-the-south-african-banking-sector-paper","text":"This paper calibrates a price-mediated contagion model to the South African banking system. I rank individual banks according to their contribution to systemic risk and show the importance of cash liquidity buffers in reducing risk to fire-sale occurrences. The paper finds a critical threshold price impact parameter, which, if exceeded, makes the banking system highly unstable. source code Paper available here","title":"Koziol,  2020 (Working Paper): Stress-testing interconnected portfolios in the South African Banking Sector Paper"},{"location":"research/#koziol-riedler-2019-working-paper-euro-area-quantitative-easing-in-a-portfolio-balance-model-with-heterogeneous-agents-and-assets","text":"We present a portfolio model to study the effects of Quantitative Easing on international financial asset returns through the portfolio balance channel. Our two-country model quantifies the effect of a domestic central bank\u2019s asset purchase program on the domestic and foreign term structure of the yield curve, equity returns and the exchange rate. Paper on SSRN","title":"Koziol &amp; Riedler, 2019 (Working paper): Euro Area Quantitative Easing in a Portfolio Balance Model with Heterogeneous Agents and Assets"},{"location":"blog/2020/Catalinaconda/","text":"Catalina, zsh, bash and conda environments I upgraded my operating system to Mac OS Catalina recently and found myself having to deal with strange python behaviour. One important change on Catalina vs Mojave is that the default terminal shell (=command interpreter) is now zsh and not bash anymore. So if you had python installed on Mojave or earlier versions tailored to bash, these are the adjustments you need to do: Get Oh-my-zh Configure ~.zsh_profile Configure Catalina and conda environments Why did they change to ZSH in the first place? Well, because it exhibits improvements and higher flexibility over bash. The name 'zsh' is derived from the name of Yale professor Zong Shao . Some improvements over bash include themes and plugins, automatic cd, recursive path expansion, spelling correction, approximate completion, plugin and theme support. You can find more infos on zsh here . 1.Get Oh-my-zh First step: Install homebrew (if not done already) Homebrew is a useful tool to install software on macOS operating systems and Linux with the command line. Another way of saying that it's ' a package management system'. For example in python, the package management systems are a) anaconda and b) pip, and for LaTeX, the package manger is tlmgr. For homebrew you need to a) install Xcode via Mac App Store b) install command line tools (130MB) with xcode-select --install c) install Homebrew with : /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Second step: Configure ZSH and get Oh-my-zsh Oh-my-zsh is an open source framework for managing ZSH. Do you have zsh already? echo $SHELL If yes , install Oh-my-zh by typing brew install wget wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh Oh-my-zsh is installed in the home directory '~/.oh-my-zsh' Next, create a new configuration for zsh. As with the bash shell, which has a configuration named '.bashrc', for zsh, we need a .zshrc configuration file. It's available in the oh-my-zsh templates directory. Copy the template .zshrc.zsh-template configuration file to the home directory .zshrc and apply the configuration by running the source command: cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc source ~/.zshrc cd ~/.oh-my-zsh/themes/ ls -a vim ~/.zshrc and then change the theme you want, e.g. ZSH_THEME='risto' We need to configure the .zshr for python Which python is in your PATH? (Or which exectuable is triggered when typing python ) which python MODIFY PATH export PATH=\"the-directory-with/anaconda3/bin:$PATH\" However, we are not done yet. What would happen now is that every time I started a new terminal session, I needed to adjust my PATH variable. Annoying! I found the solution on overflow : You can find the entire anaconda3 environment in a shortcut link named 'Relocated Items' on your desktop. It appears as though the upgrade to Catalina does not allow the Conda environment to be installed under a user directory now likely having to do with the new system volume move to a read-only partition. This issue has been opened as far back as June 10th, I am a little disappointed that it was not resolved before the Catalina upgrade came around. There is a solution that appears to work without losing your environment, see this link : (optional) Copy the folder anaconda3 located in Relocated Items to /Users/myname/ Open Terminal Enter: export PATH='/Users/myname/anaconda3/bin:$PATH' Enter: conda init zsh","title":"Catalina"},{"location":"blog/2020/Catalinaconda/#catalina-zsh-bash-and-conda-environments","text":"I upgraded my operating system to Mac OS Catalina recently and found myself having to deal with strange python behaviour. One important change on Catalina vs Mojave is that the default terminal shell (=command interpreter) is now zsh and not bash anymore. So if you had python installed on Mojave or earlier versions tailored to bash, these are the adjustments you need to do: Get Oh-my-zh Configure ~.zsh_profile Configure Catalina and conda environments Why did they change to ZSH in the first place? Well, because it exhibits improvements and higher flexibility over bash. The name 'zsh' is derived from the name of Yale professor Zong Shao . Some improvements over bash include themes and plugins, automatic cd, recursive path expansion, spelling correction, approximate completion, plugin and theme support. You can find more infos on zsh here .","title":"Catalina, zsh, bash and conda environments"},{"location":"blog/2020/Catalinaconda/#1get-oh-my-zh","text":"","title":"1.Get Oh-my-zh"},{"location":"blog/2020/Catalinaconda/#first-step-install-homebrew-if-not-done-already","text":"Homebrew is a useful tool to install software on macOS operating systems and Linux with the command line. Another way of saying that it's ' a package management system'. For example in python, the package management systems are a) anaconda and b) pip, and for LaTeX, the package manger is tlmgr. For homebrew you need to a) install Xcode via Mac App Store b) install command line tools (130MB) with xcode-select --install c) install Homebrew with : /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"","title":"First step: Install homebrew (if not done already)"},{"location":"blog/2020/Catalinaconda/#second-step-configure-zsh-and-get-oh-my-zsh","text":"Oh-my-zsh is an open source framework for managing ZSH. Do you have zsh already? echo $SHELL If yes , install Oh-my-zh by typing brew install wget wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh Oh-my-zsh is installed in the home directory '~/.oh-my-zsh' Next, create a new configuration for zsh. As with the bash shell, which has a configuration named '.bashrc', for zsh, we need a .zshrc configuration file. It's available in the oh-my-zsh templates directory. Copy the template .zshrc.zsh-template configuration file to the home directory .zshrc and apply the configuration by running the source command: cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc source ~/.zshrc cd ~/.oh-my-zsh/themes/ ls -a vim ~/.zshrc and then change the theme you want, e.g. ZSH_THEME='risto'","title":"Second step: Configure ZSH and get Oh-my-zsh"},{"location":"blog/2020/Catalinaconda/#we-need-to-configure-the-zshr-for-python","text":"Which python is in your PATH? (Or which exectuable is triggered when typing python ) which python MODIFY PATH export PATH=\"the-directory-with/anaconda3/bin:$PATH\" However, we are not done yet. What would happen now is that every time I started a new terminal session, I needed to adjust my PATH variable. Annoying! I found the solution on overflow : You can find the entire anaconda3 environment in a shortcut link named 'Relocated Items' on your desktop. It appears as though the upgrade to Catalina does not allow the Conda environment to be installed under a user directory now likely having to do with the new system volume move to a read-only partition. This issue has been opened as far back as June 10th, I am a little disappointed that it was not resolved before the Catalina upgrade came around. There is a solution that appears to work without losing your environment, see this link : (optional) Copy the folder anaconda3 located in Relocated Items to /Users/myname/ Open Terminal Enter: export PATH='/Users/myname/anaconda3/bin:$PATH' Enter: conda init zsh","title":"We need to configure the .zshr  for python"},{"location":"blog/2020/ba900-download/","text":"Download and analyse SARB BA900 forms This is a description of the ba900 programm at https://github.com/t1nak/ba900/ Attention: requires python 2.7! Yes, the old one. I coded this up a long time ago. I will probably transcribe it to python 3 at some point. So long, you must create a python environment with python=2.7 because basestring is missing in python 3. Other dependencies: lxml and more_itertools ( pypi ) Get the South African Reserve Bank BA 900 forms Time series of SA banks' balance sheets items I provide the data as of 24 October 2020 in this repo. If you want to update and download yourself, use the following steps. Step 1 Run this snippet to create the folder data/downloaded in your project directory and one for each year available on the website: import os folder='data/downloaded' path = os.path.join(os.getcwd(),folder) years = [i for i in range(2008, 2021, 1)] for y in years: if not os.path.exists(os.path.join(path,str(y))): os.makedirs(path+str(y)) Step 2 Download all the xml files from the SARB's homepage and unzip into the corresponding folder. If I find time I will add a little selenium function to do this automatically, but it's very simple as it is :) Step 3 To convert the data from xml to dataframe run python ba900.py or the following snipped with the folder name you just created. The script will extract all data from the xml files and save as year.pkl import pandas as pd from collections import Counter from lxml import etree import numpy as np import os, sys sys.path.insert(0,os.getcwd()) from extract_data import * from iteration import * import pickle input = os.path.join(os.getcwd(), 'data/downloaded/') output = os.path.join(os.getcwd(), 'data/output/') if not os.path.exists(input): os.mkdir(input) if not os.path.exists(output): os.mkdir(output) years = [ 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020] data = save_yearly_data(years, folder) #This will save all years as one giant pickle (approx. 2GB) #Uncomment if you want this # filename = os.path.join(folder, 'all_data.pickle') # with open(filename, 'wb') as f: # pickle.dump(data, f) print('All done. Have fun analysing.')nt('All done. Have fun analysing.') After calling ba900.py you should see something like this: So the banks and their respective monthly data are being processed. It takes about 30 to 40 min to run all the bank and years (2008 to 2020). Step 4 Now you have the data - great. However, analysis of the data requires good understanding of the ba900 forms. Here is a csv for ABSA, December 2008. - download link There are 18 tables and 383 unique Item numbers . For example asset side positions are item numbers 103 to 277 . After familiarising yourself with the structure, you can load the pickled data and look at the time series with something like this #load data import os import pandas as pd import matplotlib.pyplot as plt import numpy as np import datetime file_list=[] for filename in os.listdir('./data/output/'): if filename.endswith(\".pkl\"): unpickle = './data/output/'+str(filename) print(unpickle) file_list.append(pd.read_pickle(unpickle)) MASTER = pd.concat(file_list) MASTER['time'] = pd.to_datetime(MASTER['time']) #print all column names print(MASTER.columns) Print all banks in the data set with print(MASTER['InstitutionDescription'].unique()) Look at ABSA bank asset portfolio weights: #need assets_to_weights.py from assets_to_weights import * transform=tranformer() years=[str(i) for i in range(2008,2020, 1)] months=[\"{:02d}\".format(i) for i in range(1,13)] helper_dict={} for y in years: helper_dict[y]=months BANK = MASTER[MASTER['InstitutionDescription']=='ABSA BANK LTD '] BANK['Value'] = pd.to_numeric(BANK['Value']) bank_year_and_month = [] for key in helper_dict.keys(): for month in helper_dict[key]: df_bank_key_month = BANK[(BANK['TheYear']==key)&(BANK['TheMonth']==month)] # print((month),(key),df_bank_key_month ) try: bank_year_and_month.append(transform.get_pf_weights_by_bank_29(df_bank_key_month)) except: pass absa=pd.concat(bank_year_and_month) # we only need one entry per month - so select column code 7 and itemnumber 2 for example absa_weights=absa[(absa['ColumnCode']=='7')&(absa['ItemNumber']=='2')] absa_weights.index=absa_weights.time f = plt.figure() plt.title('ABSA portfolio weights', color='black') for column in absa_weights[absa.columns[-19:-10]]: absa_weights[column].plot(legend=column, ax=f.gca()) plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) plt.show() So you can see the gradual decline in household mortgage credit in ABSA's balance sheet as a share of its total balance sheet size. For the top 4 banks: # show portfolio weights for absa's credit book from assets_to_weights import * transform=tranformer() years=[str(i) for i in range(2008,2021, 1)] months=[\"{:02d}\".format(i) for i in range(1,13)] top5=[ 'ABSA BANK LTD ',\\ 'THE STANDARD BANK OF S A LTD ',\\ MASTER[MASTER['InstitutionDescription'].str.contains('NEDBANK')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('FIRSTRAND')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('CAPITEC BANK')]['InstitutionDescription'].values[0] ] df=MASTER[MASTER.InstitutionDescription.isin(top5)] arr = np.empty((0,len(df[df['InstitutionDescription'].str.contains('ABSA BANK LTD ')].time.unique()))) for b in top5: test=transform.get_bankdata(years,months,df, b) # # we only need one entry per month - so select column code 7 and itemnumber 2 for example test_weights=test[(test['ColumnCode']=='7')&(test['ItemNumber']=='2')] test_weights.index=test_weights.time # f = plt.figure() # plt.title('ABSA portfolio weights', color='black') # for column in test_weights[test.columns[-19:-10]]: # test_weights[column].plot(legend=column, ax=f.gca()) # plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) # plt.show() weights_only=test_weights[test_weights.columns[-29:]] d=weights_only.filter(like='Household_mortgages').squeeze().values arr=np.vstack((arr,d)) import matplotlib.pyplot as plt top5=[ 'ABSA BANK LTD ',\\ 'THE STANDARD BANK OF S A LTD ',\\ MASTER[MASTER['InstitutionDescription'].str.contains('NEDBANK')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('FIRSTRAND')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('CAPITEC BANK')]['InstitutionDescription'].values[0] ] f = plt.figure() plt.title('Household mortgage portfolio weights', color='black') for i,name in zip(arr[:-1],top5[:-1]): plt.plot(test_weights.time,i , label=name) plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) plt.show() Euclidean distances betwenn top 10 banks (dec 2018): You can check some examples in the exame_analyse.ipynb notebook. Write me if you have questions. There is a lot of scope to make this more user-friendly and if I get more feedback I will gladly do so. However, for my own purposes it has been sufficient like it is.","title":"Analyse and Download SARB BA900"},{"location":"blog/2020/ba900-download/#download-and-analyse-sarb-ba900-forms","text":"This is a description of the ba900 programm at https://github.com/t1nak/ba900/ Attention: requires python 2.7! Yes, the old one. I coded this up a long time ago. I will probably transcribe it to python 3 at some point. So long, you must create a python environment with python=2.7 because basestring is missing in python 3. Other dependencies: lxml and more_itertools ( pypi )","title":"Download and analyse SARB BA900 forms"},{"location":"blog/2020/ba900-download/#get-the-south-african-reserve-bank-ba-900-forms","text":"","title":"Get the South African Reserve Bank BA 900 forms"},{"location":"blog/2020/ba900-download/#time-series-of-sa-banks-balance-sheets-items","text":"I provide the data as of 24 October 2020 in this repo. If you want to update and download yourself, use the following steps.","title":"Time series of SA banks' balance sheets items"},{"location":"blog/2020/ba900-download/#step-1","text":"Run this snippet to create the folder data/downloaded in your project directory and one for each year available on the website: import os folder='data/downloaded' path = os.path.join(os.getcwd(),folder) years = [i for i in range(2008, 2021, 1)] for y in years: if not os.path.exists(os.path.join(path,str(y))): os.makedirs(path+str(y))","title":"Step 1"},{"location":"blog/2020/ba900-download/#step-2","text":"Download all the xml files from the SARB's homepage and unzip into the corresponding folder. If I find time I will add a little selenium function to do this automatically, but it's very simple as it is :)","title":"Step 2"},{"location":"blog/2020/ba900-download/#step-3","text":"To convert the data from xml to dataframe run python ba900.py or the following snipped with the folder name you just created. The script will extract all data from the xml files and save as year.pkl import pandas as pd from collections import Counter from lxml import etree import numpy as np import os, sys sys.path.insert(0,os.getcwd()) from extract_data import * from iteration import * import pickle input = os.path.join(os.getcwd(), 'data/downloaded/') output = os.path.join(os.getcwd(), 'data/output/') if not os.path.exists(input): os.mkdir(input) if not os.path.exists(output): os.mkdir(output) years = [ 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020] data = save_yearly_data(years, folder) #This will save all years as one giant pickle (approx. 2GB) #Uncomment if you want this # filename = os.path.join(folder, 'all_data.pickle') # with open(filename, 'wb') as f: # pickle.dump(data, f) print('All done. Have fun analysing.')nt('All done. Have fun analysing.') After calling ba900.py you should see something like this: So the banks and their respective monthly data are being processed. It takes about 30 to 40 min to run all the bank and years (2008 to 2020).","title":"Step 3"},{"location":"blog/2020/ba900-download/#step-4","text":"Now you have the data - great. However, analysis of the data requires good understanding of the ba900 forms. Here is a csv for ABSA, December 2008. - download link There are 18 tables and 383 unique Item numbers . For example asset side positions are item numbers 103 to 277 . After familiarising yourself with the structure, you can load the pickled data and look at the time series with something like this #load data import os import pandas as pd import matplotlib.pyplot as plt import numpy as np import datetime file_list=[] for filename in os.listdir('./data/output/'): if filename.endswith(\".pkl\"): unpickle = './data/output/'+str(filename) print(unpickle) file_list.append(pd.read_pickle(unpickle)) MASTER = pd.concat(file_list) MASTER['time'] = pd.to_datetime(MASTER['time']) #print all column names print(MASTER.columns) Print all banks in the data set with print(MASTER['InstitutionDescription'].unique()) Look at ABSA bank asset portfolio weights: #need assets_to_weights.py from assets_to_weights import * transform=tranformer() years=[str(i) for i in range(2008,2020, 1)] months=[\"{:02d}\".format(i) for i in range(1,13)] helper_dict={} for y in years: helper_dict[y]=months BANK = MASTER[MASTER['InstitutionDescription']=='ABSA BANK LTD '] BANK['Value'] = pd.to_numeric(BANK['Value']) bank_year_and_month = [] for key in helper_dict.keys(): for month in helper_dict[key]: df_bank_key_month = BANK[(BANK['TheYear']==key)&(BANK['TheMonth']==month)] # print((month),(key),df_bank_key_month ) try: bank_year_and_month.append(transform.get_pf_weights_by_bank_29(df_bank_key_month)) except: pass absa=pd.concat(bank_year_and_month) # we only need one entry per month - so select column code 7 and itemnumber 2 for example absa_weights=absa[(absa['ColumnCode']=='7')&(absa['ItemNumber']=='2')] absa_weights.index=absa_weights.time f = plt.figure() plt.title('ABSA portfolio weights', color='black') for column in absa_weights[absa.columns[-19:-10]]: absa_weights[column].plot(legend=column, ax=f.gca()) plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) plt.show() So you can see the gradual decline in household mortgage credit in ABSA's balance sheet as a share of its total balance sheet size. For the top 4 banks: # show portfolio weights for absa's credit book from assets_to_weights import * transform=tranformer() years=[str(i) for i in range(2008,2021, 1)] months=[\"{:02d}\".format(i) for i in range(1,13)] top5=[ 'ABSA BANK LTD ',\\ 'THE STANDARD BANK OF S A LTD ',\\ MASTER[MASTER['InstitutionDescription'].str.contains('NEDBANK')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('FIRSTRAND')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('CAPITEC BANK')]['InstitutionDescription'].values[0] ] df=MASTER[MASTER.InstitutionDescription.isin(top5)] arr = np.empty((0,len(df[df['InstitutionDescription'].str.contains('ABSA BANK LTD ')].time.unique()))) for b in top5: test=transform.get_bankdata(years,months,df, b) # # we only need one entry per month - so select column code 7 and itemnumber 2 for example test_weights=test[(test['ColumnCode']=='7')&(test['ItemNumber']=='2')] test_weights.index=test_weights.time # f = plt.figure() # plt.title('ABSA portfolio weights', color='black') # for column in test_weights[test.columns[-19:-10]]: # test_weights[column].plot(legend=column, ax=f.gca()) # plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) # plt.show() weights_only=test_weights[test_weights.columns[-29:]] d=weights_only.filter(like='Household_mortgages').squeeze().values arr=np.vstack((arr,d)) import matplotlib.pyplot as plt top5=[ 'ABSA BANK LTD ',\\ 'THE STANDARD BANK OF S A LTD ',\\ MASTER[MASTER['InstitutionDescription'].str.contains('NEDBANK')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('FIRSTRAND')]['InstitutionDescription'].values[0],\\ MASTER[MASTER['InstitutionDescription'].str.contains('CAPITEC BANK')]['InstitutionDescription'].values[0] ] f = plt.figure() plt.title('Household mortgage portfolio weights', color='black') for i,name in zip(arr[:-1],top5[:-1]): plt.plot(test_weights.time,i , label=name) plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) plt.show() Euclidean distances betwenn top 10 banks (dec 2018): You can check some examples in the exame_analyse.ipynb notebook. Write me if you have questions. There is a lot of scope to make this more user-friendly and if I get more feedback I will gladly do so. However, for my own purposes it has been sufficient like it is.","title":"Step 4"},{"location":"blog/2020/cost/","text":"Cost mapping for friction surface Here are a few notes on one of my projects on friction surface mapping Goal Have study area raster/statistics where each pixel represents the minimum travel time to market - Problem is generalisable to incorparating additional components, e.g. climate and seasonality to get perishable pressure of crop location Background - What is it These papers demonstrated how to do friction surface mapping on a global scale (in R) Weiss, D., Nelson, A., Gibson, H. et al. A global map of travel time to cities to assess inequalities in accessibility in 2015. Nature 553, 333\u2013336 (2018). - travel time to cities with more than 50,000 people Weiss, D.J., Nelson, A., Vargas-Ruiz, C.A. et al. Global maps of travel time to healthcare facilities . Nat Med (2020) Least-cost-path algorithm developed by Dijkstra (1959) for use in graphs algorithm coded from Google Earth Engine and gDistance package Data: roads, railways, navigable waterways, bodies of water, land cover types, elevation, slope angle, and national borders roads location data (OSM, Google) road speed data from OSM Requirements - What do we need data structure (image/raster) in which each pixel represents the cost per meter to traverse it Impact factors in human travel Traffic network infrastructure Terrain specifics (where no infrastructure exists) Tools - How to do it When doing cost modelling, we're dealing with projected surfaces. When you accumulate costs over large projected surfaces, the pixel size represent different distances - but we need equal distance from any location! e.g on a latitude, longitude map of the world, pixel at the equator is very different in size and shape than in Minnesota The only way to do this is on a sphere gdistance e.g. calculates great circle distances between every pair of location; this distance represents the shortest line between two points, taking into account the curvatur of the earth. It turns out to be the best predictor of genetic distance with small scale (UTM) ARGIS is fine, otherwise there will be distortions Google earth engine image.cumulativeCost() to compute a cost map where every pixel contains the total cost of the lowest cost path to the nearest source location source R package gdistance Etten, Jacob van. \"R package gdistance: distances and routes on geographical grids.\u201d Journal of Statistical Software (2017) the accCost() function Superb handling of adjacency (transition matrix) and cumulative cost mapping to central point geoCorrection function of the leastCost algorithm corrects for distance distortions associated with data in a geographic coordinate system corrects for \u2018true\u2019 local distance","title":"Cost mapping for friction surfaces"},{"location":"blog/2020/cost/#cost-mapping-for-friction-surface","text":"Here are a few notes on one of my projects on friction surface mapping","title":"Cost mapping for friction surface"},{"location":"blog/2020/cost/#goal","text":"Have study area raster/statistics where each pixel represents the minimum travel time to market - Problem is generalisable to incorparating additional components, e.g. climate and seasonality to get perishable pressure of crop location","title":"Goal"},{"location":"blog/2020/cost/#background-what-is-it","text":"These papers demonstrated how to do friction surface mapping on a global scale (in R) Weiss, D., Nelson, A., Gibson, H. et al. A global map of travel time to cities to assess inequalities in accessibility in 2015. Nature 553, 333\u2013336 (2018). - travel time to cities with more than 50,000 people Weiss, D.J., Nelson, A., Vargas-Ruiz, C.A. et al. Global maps of travel time to healthcare facilities . Nat Med (2020) Least-cost-path algorithm developed by Dijkstra (1959) for use in graphs algorithm coded from Google Earth Engine and gDistance package Data: roads, railways, navigable waterways, bodies of water, land cover types, elevation, slope angle, and national borders roads location data (OSM, Google) road speed data from OSM","title":"Background - What is it"},{"location":"blog/2020/cost/#requirements-what-do-we-need","text":"data structure (image/raster) in which each pixel represents the cost per meter to traverse it Impact factors in human travel Traffic network infrastructure Terrain specifics (where no infrastructure exists)","title":"Requirements - What do we need"},{"location":"blog/2020/cost/#tools-how-to-do-it","text":"When doing cost modelling, we're dealing with projected surfaces. When you accumulate costs over large projected surfaces, the pixel size represent different distances - but we need equal distance from any location! e.g on a latitude, longitude map of the world, pixel at the equator is very different in size and shape than in Minnesota The only way to do this is on a sphere gdistance e.g. calculates great circle distances between every pair of location; this distance represents the shortest line between two points, taking into account the curvatur of the earth. It turns out to be the best predictor of genetic distance with small scale (UTM) ARGIS is fine, otherwise there will be distortions","title":"Tools - How to do it"},{"location":"blog/2020/cost/#google-earth-engine","text":"image.cumulativeCost() to compute a cost map where every pixel contains the total cost of the lowest cost path to the nearest source location source","title":"Google earth engine"},{"location":"blog/2020/cost/#r-package-gdistance","text":"Etten, Jacob van. \"R package gdistance: distances and routes on geographical grids.\u201d Journal of Statistical Software (2017) the accCost() function Superb handling of adjacency (transition matrix) and cumulative cost mapping to central point geoCorrection function of the leastCost algorithm corrects for distance distortions associated with data in a geographic coordinate system corrects for \u2018true\u2019 local distance","title":"R package gdistance"},{"location":"blog/2020/ee/","text":"Google earth engine functionality image.cumulativeCost() to compute a cost map where every pixel contains the total cost of the lowest cost path to the nearest source location source Need: - need image in which each pixel represents the cost per meter to traverse it - Impact factors in human travel - Traffic network infrastructure - Terrain specifics (where no infrastructure exists)","title":"Google earth engine functionality"},{"location":"blog/2020/ee/#google-earth-engine-functionality","text":"image.cumulativeCost() to compute a cost map where every pixel contains the total cost of the lowest cost path to the nearest source location source Need: - need image in which each pixel represents the cost per meter to traverse it - Impact factors in human travel - Traffic network infrastructure - Terrain specifics (where no infrastructure exists)","title":"Google earth engine functionality"},{"location":"blog/2020/geogems1/","text":"Table of Contents Examples (in jupyter notebook) 1. Start geogems and load images Show wet and dry season from sentinel 2 images for Malawi Dry season Wet season 2. Load landcover satellite images !pip search geogems geogems (0.1.2) - GEMS friction surface helper functions INSTALLED: 0.1.2 (latest) Show Wet and Dry season sentinel images for Malawi import ee import geogems # ee.Authenticate() #uncomment for first run to authorise with google ee.Initialize() area= 'Malawi' lat = -12.8018637 lon = 33.4752805 #time series dry_season = ('2019-05-01' , '2019-10-30') wet_season = ('2019-11-01', '2020-04-30') #load the earth engine helper functions geography_object = geogems.map.ee_helper() #load the map map_malawai=geography_object.show(lat, lon, 6.5) Latitude -12.8018637, longtitude 33.4752805, zoom 6.5 #get earth engines objects (bounds and sentinet images) bounds=geography_object.get_country_polygon_by_name(ee, area) #pass in country name image_dry,image_wet,trueColor_palette= geography_object.\\ get_sentinel_dry_wet_season(dry_season,wet_season ,ee,bounds) # show map map_malawai # Malawi polygon returned as FeatureCollection Sentinuel images returned Map(bottom=17834.0, center=[-12.8018637, 33.4752805], controls=(WidgetControl(options=['position'], widget=HBo\u2026 wet season #Add image map_malawai.addLayer(image_wet, trueColor_palette, '2019 Dry season true color'); map_malawai.addLayerControl() #load modis landcover maplc_malawai=geography_object.show(lat, lon, 6.5) image_lc,legend= geography_object.get_Modis_MCD12Q1_landcover_2013_01_01(ee,bounds) maplc_malawai Map(center=[-12.8018637, 33.4752805], controls=(WidgetControl(options=['position'], widget=HBox(children=(Togg\u2026 [] (https://warehouse-camo.ingress.cmh1.psfhosted.org/de3ba43fc8eb9586a1c774ebfbbece563cbcc9bd/68747470733a2f2f6769746875622e636f6d2f74316e616b2f67656f67656d732f626c6f622f6d61696e2f706963732f6d6f6469735f6c632e706e673f7261773d74727565) maplc_malawai.addLayer(image_lc, {}, 'MODIS Land Cover') maplc_malawai.add_legend(legend_title=\"MODIS Global Land Cover\", legend_dict=legend)","title":"Geogems part 1"},{"location":"blog/2020/geogems1/#table-of-contents","text":"Examples (in jupyter notebook) 1. Start geogems and load images Show wet and dry season from sentinel 2 images for Malawi Dry season Wet season 2. Load landcover satellite images !pip search geogems geogems (0.1.2) - GEMS friction surface helper functions INSTALLED: 0.1.2 (latest)","title":"Table of Contents"},{"location":"blog/2020/geogems1/#show-wet-and-dry-season-sentinel-images-for-malawi","text":"import ee import geogems # ee.Authenticate() #uncomment for first run to authorise with google ee.Initialize() area= 'Malawi' lat = -12.8018637 lon = 33.4752805 #time series dry_season = ('2019-05-01' , '2019-10-30') wet_season = ('2019-11-01', '2020-04-30') #load the earth engine helper functions geography_object = geogems.map.ee_helper() #load the map map_malawai=geography_object.show(lat, lon, 6.5) Latitude -12.8018637, longtitude 33.4752805, zoom 6.5 #get earth engines objects (bounds and sentinet images) bounds=geography_object.get_country_polygon_by_name(ee, area) #pass in country name image_dry,image_wet,trueColor_palette= geography_object.\\ get_sentinel_dry_wet_season(dry_season,wet_season ,ee,bounds) # show map map_malawai # Malawi polygon returned as FeatureCollection Sentinuel images returned Map(bottom=17834.0, center=[-12.8018637, 33.4752805], controls=(WidgetControl(options=['position'], widget=HBo\u2026 wet season #Add image map_malawai.addLayer(image_wet, trueColor_palette, '2019 Dry season true color'); map_malawai.addLayerControl() #load modis landcover maplc_malawai=geography_object.show(lat, lon, 6.5) image_lc,legend= geography_object.get_Modis_MCD12Q1_landcover_2013_01_01(ee,bounds) maplc_malawai Map(center=[-12.8018637, 33.4752805], controls=(WidgetControl(options=['position'], widget=HBox(children=(Togg\u2026 [] (https://warehouse-camo.ingress.cmh1.psfhosted.org/de3ba43fc8eb9586a1c774ebfbbece563cbcc9bd/68747470733a2f2f6769746875622e636f6d2f74316e616b2f67656f67656d732f626c6f622f6d61696e2f706963732f6d6f6469735f6c632e706e673f7261773d74727565) maplc_malawai.addLayer(image_lc, {}, 'MODIS Land Cover') maplc_malawai.add_legend(legend_title=\"MODIS Global Land Cover\", legend_dict=legend)","title":"Show Wet and Dry season sentinel images for Malawi"},{"location":"blog/2020/h3intro/","text":"H3: Uber's open source hierarchical hexagonal grid for the world The geospatial community is contemplating the pro's and con's of Uber's H3 discrete global grid system. The quick facts: A global geospatial indexing system, i.e. multi-precision tiling of the world with hierarchical linear indexes to organize data 16 resolutions The grid approximates geo features such as polygons or points, avoiding expensive geospatial operations --> exhibits great scaling behavior It's a discrete global grid system , i.e. it breaks up the world into discrete cells, where every position in the world has a cell identifier associated with it H3 is a system of grids . All 16 grid resolutions of the grid relate to each other; the resolution 10 grid is created by subdividing the resolution 9 grid and so forth This table show the different spatial dimensions per resolution (0-15 is 1-16) Nice A hexagonal system is more convenient for modeling spatial transformations, because neighbors are equal distant Uniform adjency neat property: all neighbors are the same distance apart When using H3, a point, a polygon, or any kind of shape gets coordinates, plus a resolution, and an indexing operation. The indexing operation is exactly at that resolution. The result is a list of indexes/identifiers for every single point of interest (e.g. building, market, hospital) Usecase: Binning, database look-ups and categorisation When working with large data sets, a great way to simplify the visual representation of complex point data is to spatially aggregate the points into bin regions so that you can look at groups of data instead of the individual points source You can map a data source from the H3 identifier to a metric value for that cell and do database lookups. For example, you can answer the question: \"I'm in this area, I want to know how many buildings there are around me\" It\u2019s just a matter of checking coordinates, finding the H3 identifier, and then looking that up in a database that is set up to map from the H3 identifier to that metric value Great use case: binning and categorisations source Binning mitigates variable resolution of source data and reduces computational complexity of feature engineering When using a binning strategy to aggregate base points geographically, h3 is a great tool For example: H3 generates an H3 bin from a set of coordinates, and a boundrary shape for an H3 bin. In conjunction, these functions allow us to assign a bin to each point of interest, and then use that bin's geography to extract geodata from the raster datasets With H3 all cells within a defined search distance can be looked up easily It doesn't really know what you put next to it inside of your database. You could create a database in a Big Data system where one of those columns is an H3 identifier. The other columns can be whatever you want them to be, within the constraints of that database. You can attach categorical or numerical metrics to it There are different ways to get a raster into the H3 grid. One is by sampling from the raster. We take an H3 grid, and we find some points we want to sample from the raster. Then we use that to move that raster data into the grid. Another way is by taking all the pixels in that raster and finding the associated H3 cell and moving the data in that way. Unify data spatially However, the hexagonal hierarchy could be a disadvantage when dividing hierarchies into smaller units. Spatial errors are not present, for instance, in squares As a good example, in the US, we have customer data in zip code format, and we have demographics data in a format we'd get from the US Census Bureau. They use different geometries which the H3 grid joins together. We can also bring in raster, GPS tracks and all these different data and unify them into a simple analysis that the user is doing. It makes it a lot easier to access geospatial data. Coversion in and out of H3 The ability to convert data back out of hexagons is just as critical. This allows you to transform between that sample of the US Census of geographies to zip codes. We're able to use a hexagon grid to transform remarkably efficiently from one polygon geometry to another. Users can do their analysis in a geometry that they're comfortable with and that they feel useful. Functions: polyfill H3 polyfill takes an input polygon and finds the H3 indexes which cover it. So you can take a given input polygon that you re interested in and then find the index H3 grid that covers it. So you use the system from within a geometry rather than having to use the grid as the first instance of coverage. Radius Lookup : You can use H3 for an efficient radius lookup, as long as you're willing to approximate the circle as a k-ring of hexagons (roughly, a large hexagonal shape). This is much faster than a true search by Haversine distance, but less accurate. h3_distance(H3Index origin, H3Index h3) Get the grid distance between H3 addresses. Returns the distance in grid cells between the two indexes. Returns a negative number if finding the distance failed. Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions. #haversine distance calc example: def haversine_dist(lon_src, lat_src, lon_dst, lat_dst): '''returns distance between GPS points, measured in meters''' lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(np.radians, [lon_src, lat_src, lon_dst, lat_dst]) dlon = lon2_rad - lon1_rad dlat = lat2_rad - lat1_rad a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * \\ np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2 c = 2 * np.arcsin(np.sqrt(a)) km = 6367 * c return km * 1000 Fantastic notebook here Check out this excellent podcast by mapscaping Scaling Geospatial Workloads Usecase for friction modelling If we are handling shape files at large scale h3 might be useful","title":"Uber's H3 grid"},{"location":"blog/2020/h3intro/#h3-ubers-open-source-hierarchical-hexagonal-grid-for-the-world","text":"The geospatial community is contemplating the pro's and con's of Uber's H3 discrete global grid system. The quick facts: A global geospatial indexing system, i.e. multi-precision tiling of the world with hierarchical linear indexes to organize data 16 resolutions The grid approximates geo features such as polygons or points, avoiding expensive geospatial operations --> exhibits great scaling behavior It's a discrete global grid system , i.e. it breaks up the world into discrete cells, where every position in the world has a cell identifier associated with it H3 is a system of grids . All 16 grid resolutions of the grid relate to each other; the resolution 10 grid is created by subdividing the resolution 9 grid and so forth This table show the different spatial dimensions per resolution (0-15 is 1-16) Nice A hexagonal system is more convenient for modeling spatial transformations, because neighbors are equal distant Uniform adjency neat property: all neighbors are the same distance apart When using H3, a point, a polygon, or any kind of shape gets coordinates, plus a resolution, and an indexing operation. The indexing operation is exactly at that resolution. The result is a list of indexes/identifiers for every single point of interest (e.g. building, market, hospital)","title":"H3: Uber's open source hierarchical hexagonal grid for the world"},{"location":"blog/2020/h3intro/#usecase-binning-database-look-ups-and-categorisation","text":"When working with large data sets, a great way to simplify the visual representation of complex point data is to spatially aggregate the points into bin regions so that you can look at groups of data instead of the individual points source You can map a data source from the H3 identifier to a metric value for that cell and do database lookups. For example, you can answer the question: \"I'm in this area, I want to know how many buildings there are around me\" It\u2019s just a matter of checking coordinates, finding the H3 identifier, and then looking that up in a database that is set up to map from the H3 identifier to that metric value Great use case: binning and categorisations source Binning mitigates variable resolution of source data and reduces computational complexity of feature engineering When using a binning strategy to aggregate base points geographically, h3 is a great tool For example: H3 generates an H3 bin from a set of coordinates, and a boundrary shape for an H3 bin. In conjunction, these functions allow us to assign a bin to each point of interest, and then use that bin's geography to extract geodata from the raster datasets With H3 all cells within a defined search distance can be looked up easily It doesn't really know what you put next to it inside of your database. You could create a database in a Big Data system where one of those columns is an H3 identifier. The other columns can be whatever you want them to be, within the constraints of that database. You can attach categorical or numerical metrics to it There are different ways to get a raster into the H3 grid. One is by sampling from the raster. We take an H3 grid, and we find some points we want to sample from the raster. Then we use that to move that raster data into the grid. Another way is by taking all the pixels in that raster and finding the associated H3 cell and moving the data in that way.","title":"Usecase: Binning, database look-ups and categorisation"},{"location":"blog/2020/h3intro/#unify-data-spatially","text":"However, the hexagonal hierarchy could be a disadvantage when dividing hierarchies into smaller units. Spatial errors are not present, for instance, in squares As a good example, in the US, we have customer data in zip code format, and we have demographics data in a format we'd get from the US Census Bureau. They use different geometries which the H3 grid joins together. We can also bring in raster, GPS tracks and all these different data and unify them into a simple analysis that the user is doing. It makes it a lot easier to access geospatial data.","title":"Unify data spatially"},{"location":"blog/2020/h3intro/#coversion-in-and-out-of-h3","text":"The ability to convert data back out of hexagons is just as critical. This allows you to transform between that sample of the US Census of geographies to zip codes. We're able to use a hexagon grid to transform remarkably efficiently from one polygon geometry to another. Users can do their analysis in a geometry that they're comfortable with and that they feel useful. Functions: polyfill H3 polyfill takes an input polygon and finds the H3 indexes which cover it. So you can take a given input polygon that you re interested in and then find the index H3 grid that covers it. So you use the system from within a geometry rather than having to use the grid as the first instance of coverage. Radius Lookup : You can use H3 for an efficient radius lookup, as long as you're willing to approximate the circle as a k-ring of hexagons (roughly, a large hexagonal shape). This is much faster than a true search by Haversine distance, but less accurate. h3_distance(H3Index origin, H3Index h3) Get the grid distance between H3 addresses. Returns the distance in grid cells between the two indexes. Returns a negative number if finding the distance failed. Finding the distance can fail because the two indexes are not comparable (different resolutions), too far apart, or are separated by pentagonal distortion. This is the same set of limitations as the local IJ coordinate space functions. #haversine distance calc example: def haversine_dist(lon_src, lat_src, lon_dst, lat_dst): '''returns distance between GPS points, measured in meters''' lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(np.radians, [lon_src, lat_src, lon_dst, lat_dst]) dlon = lon2_rad - lon1_rad dlat = lat2_rad - lat1_rad a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * \\ np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2 c = 2 * np.arcsin(np.sqrt(a)) km = 6367 * c return km * 1000","title":"Coversion in and out of H3"},{"location":"blog/2020/h3intro/#fantastic-notebook-here","text":"","title":"Fantastic notebook here"},{"location":"blog/2020/h3intro/#check-out-this-excellent-podcast-by-mapscaping","text":"","title":"Check out this excellent podcast by  mapscaping"},{"location":"blog/2020/h3intro/#scaling-geospatial-workloads","text":"Usecase for friction modelling If we are handling shape files at large scale h3 might be useful","title":"Scaling Geospatial Workloads"},{"location":"blog/2020/installpy/","text":"Installing python on Mac with Anaconda Go to https://docs.anaconda.com/anaconda/install/ and use the standard installer. Yes it's big, but it's the easiest and smoothest way for a beginner. Tip: Always know where your python executable is ( which python in terminal) and configure your PATH! I would not recommend beginner to install python with Homebrew --> brew cask install anaconda will want to install the python distribution in /usr/local but conda was designed to be installed in $HOME --> this could lead to unpredictable behaviour!","title":"Install python"},{"location":"blog/2020/installpy/#installing-python-on-mac-with-anaconda","text":"Go to https://docs.anaconda.com/anaconda/install/ and use the standard installer. Yes it's big, but it's the easiest and smoothest way for a beginner. Tip: Always know where your python executable is ( which python in terminal) and configure your PATH! I would not recommend beginner to install python with Homebrew --> brew cask install anaconda will want to install the python distribution in /usr/local but conda was designed to be installed in $HOME --> this could lead to unpredictable behaviour!","title":"Installing python on Mac with Anaconda"}]}